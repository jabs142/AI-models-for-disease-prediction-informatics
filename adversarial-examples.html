<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Adversarial Examples in Medical AI</title>
  <style>
    * {
      margin: 0;
      padding: 0;
      box-sizing: border-box;
    }
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', sans-serif;
      background: #f5f7fa;
      color: #2c3e50;
      margin: 0;
      padding: 20px;
    }
    .container {
      max-width: 1000px;
      margin: 0 auto;
    }
    .card {
      background: white;
      border-radius: 10px;
      padding: 30px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.08);
      margin-bottom: 30px;
    }
    .card-header {
      font-weight: 600;
      font-size: 1.1em;
      margin-bottom: 25px;
      color: #1a1a1a;
    }
    h1 {
      font-size: 1.6em;
      margin-bottom: 0.4em;
      color: #1a1a1a;
    }
    .subtitle {
      color: #666;
      font-size: 0.95em;
      margin-bottom: 35px;
      line-height: 1.6;
    }
    .back-button {
      display: inline-block;
      margin-bottom: 25px;
      padding: 10px 20px;
      background: #3498db;
      color: white;
      text-decoration: none;
      border-radius: 6px;
      font-size: 0.9em;
      font-weight: 500;
      transition: background 0.2s;
    }
    .back-button:hover {
      background: #2980b9;
    }
    .control-group {
      margin-bottom: 25px;
    }
    .control-label {
      display: block;
      font-weight: 600;
      color: #2c3e50;
      margin-bottom: 10px;
      font-size: 14px;
    }
    .slider-container {
      display: flex;
      align-items: center;
      gap: 15px;
    }
    input[type="range"] {
      flex: 1;
      height: 8px;
      border-radius: 5px;
      background: #e0e0e0;
      outline: none;
      -webkit-appearance: none;
    }
    input[type="range"]::-webkit-slider-thumb {
      -webkit-appearance: none;
      appearance: none;
      width: 20px;
      height: 20px;
      border-radius: 50%;
      background: #667eea;
      cursor: pointer;
    }
    input[type="range"]::-moz-range-thumb {
      width: 20px;
      height: 20px;
      border-radius: 50%;
      background: #667eea;
      cursor: pointer;
      border: none;
    }
    .slider-value {
      font-weight: 600;
      color: #667eea;
      min-width: 60px;
      text-align: right;
      font-size: 16px;
    }
    .comparison-container {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      margin-bottom: 25px;
    }
    .scan-panel {
      border: 2px solid #e0e0e0;
      border-radius: 8px;
      padding: 20px;
      text-align: center;
    }
    .scan-panel.perturbed {
      border-color: #e74c3c;
    }
    .scan-title {
      font-weight: 600;
      color: #2c3e50;
      margin-bottom: 15px;
      font-size: 16px;
    }
    .scan-visual {
      width: 100%;
      height: 200px;
      background: #f8f9fa;
      border-radius: 8px;
      margin-bottom: 15px;
      position: relative;
      overflow: hidden;
    }
    .scan-grid {
      width: 100%;
      height: 100%;
      display: grid;
      grid-template-columns: repeat(10, 1fr);
      grid-template-rows: repeat(10, 1fr);
    }
    .scan-cell {
      border: 1px solid rgba(0,0,0,0.05);
      transition: background-color 0.3s;
    }
    .prediction-box {
      background: #f8f9fa;
      border-radius: 8px;
      padding: 15px;
    }
    .prediction-label {
      font-weight: 600;
      font-size: 18px;
      margin-bottom: 8px;
    }
    .prediction-label.healthy {
      color: #27ae60;
    }
    .prediction-label.disease {
      color: #e74c3c;
    }
    .confidence-bar {
      width: 100%;
      height: 24px;
      background: #e0e0e0;
      border-radius: 12px;
      overflow: hidden;
      position: relative;
    }
    .confidence-fill {
      height: 100%;
      transition: width 0.3s, background-color 0.3s;
      display: flex;
      align-items: center;
      justify-content: center;
      color: white;
      font-weight: 600;
      font-size: 12px;
    }
    .confidence-fill.healthy {
      background: #27ae60;
    }
    .confidence-fill.disease {
      background: #e74c3c;
    }
    .metrics-grid {
      display: grid;
      grid-template-columns: repeat(3, 1fr);
      gap: 15px;
      margin-bottom: 25px;
    }
    .metric-card {
      background: #f8f9fa;
      border-radius: 8px;
      padding: 15px;
      text-align: center;
    }
    .metric-label {
      font-size: 13px;
      color: #7f8c8d;
      margin-bottom: 8px;
      font-weight: 500;
    }
    .metric-value {
      font-size: 24px;
      font-weight: 700;
      color: #2c3e50;
    }
    .warning-box {
      background: #fff3cd;
      border-left: 4px solid #ffc107;
      padding: 15px;
      border-radius: 8px;
      margin-bottom: 25px;
    }
    .warning-box.danger {
      background: #f8d7da;
      border-left-color: #e74c3c;
    }
    .warning-title {
      font-weight: 600;
      color: #856404;
      margin-bottom: 5px;
    }
    .warning-box.danger .warning-title {
      color: #721c24;
    }
    .warning-text {
      color: #856404;
      font-size: 14px;
    }
    .warning-box.danger .warning-text {
      color: #721c24;
    }
    .collapsible-section {
      margin: 40px 0;
    }
    .collapsible-toggle {
      display: flex;
      align-items: center;
      justify-content: space-between;
      width: 100%;
      padding: 15px 20px;
      background: white;
      border: 2px solid #e0e0e0;
      border-radius: 8px;
      font-size: 0.95em;
      font-weight: 600;
      color: #495057;
      cursor: pointer;
      transition: all 0.2s;
      text-align: left;
    }
    .collapsible-toggle:hover {
      border-color: #3498db;
      color: #3498db;
      background: #f8f9fa;
    }
    .collapsible-arrow {
      font-size: 1.1em;
      transition: transform 0.2s;
      color: #3498db;
    }
    .collapsible-arrow.open {
      transform: rotate(90deg);
    }
    .collapsible-content {
      display: none;
      background: white;
      border: 1px solid #e0e0e0;
      border-radius: 8px;
      padding: 30px;
      margin-top: 12px;
      box-shadow: 0 2px 8px rgba(0,0,0,0.08);
    }
    .collapsible-content.visible {
      display: block;
    }
    .deep-dive-content h3 {
      color: #2c3e50;
      margin-top: 25px;
      margin-bottom: 12px;
      font-size: 1.05em;
    }
    .deep-dive-content h3:first-child {
      margin-top: 0;
    }
    .deep-dive-content p {
      margin-bottom: 1em;
      color: #495057;
      line-height: 1.7;
    }
    .deep-dive-content ul {
      padding-left: 25px;
      margin-bottom: 1em;
      line-height: 1.6;
    }
    .deep-dive-content li {
      margin-bottom: 6px;
    }
    .key-takeaway {
      background: #e8f5e9;
      padding: 18px 20px;
      border-left: 4px solid #28a745;
      border-radius: 6px;
      margin-top: 25px;
    }
  </style>
</head>
<body>
  <div class="container">
    <a href="index.html" class="back-button">‚Üê Back to Home</a>

    <h1>üéØ Adversarial Examples in Medical AI</h1>
    <div class="subtitle">
      <strong>What this teaches:</strong> Small, targeted perturbations to medical scans can cause AI systems to misclassify with high confidence. Watch how increasing perturbation strength fools the model into detecting disease in a healthy scan.<br>
      <strong>Why it matters:</strong> AI systems must be robust against adversarial attacks and natural variations. Understanding these vulnerabilities is critical for safe deployment in healthcare.
    </div>

    <div class="card">
      <div class="card-header">Interactive Adversarial Attack Demonstration</div>

      <div class="control-group">
        <label class="control-label">Perturbation Strength</label>
        <div class="slider-container">
          <input type="range" id="perturbationSlider" min="0" max="100" value="0" step="1">
          <span class="slider-value" id="perturbationValue">0%</span>
        </div>
      </div>

      <div id="warningBox"></div>

      <div class="comparison-container">
        <div class="scan-panel">
          <div class="scan-title">Original Scan</div>
          <div class="scan-visual">
            <canvas id="originalCanvas" width="300" height="200"></canvas>
          </div>
          <div class="prediction-box">
            <div class="prediction-label healthy">Prediction: Healthy</div>
            <div class="confidence-bar">
              <div class="confidence-fill healthy" style="width: 92%;">92% Confidence</div>
            </div>
          </div>
        </div>

        <div class="scan-panel perturbed">
          <div class="scan-title">Perturbed Scan (Adversarial)</div>
          <div class="scan-visual">
            <canvas id="perturbedCanvas" width="300" height="200"></canvas>
          </div>
          <div class="prediction-box" id="perturbedPrediction">
            <div class="prediction-label healthy">Prediction: Healthy</div>
            <div class="confidence-bar">
              <div class="confidence-fill healthy" style="width: 92%;">92% Confidence</div>
            </div>
          </div>
        </div>
      </div>

      <div class="metrics-grid">
        <div class="metric-card">
          <div class="metric-label">Perturbation Level</div>
          <div class="metric-value" id="metricPerturbation">0%</div>
        </div>
        <div class="metric-card">
          <div class="metric-label">Pixels Changed</div>
          <div class="metric-value" id="metricPixels">0</div>
        </div>
        <div class="metric-card">
          <div class="metric-label">Model Fooled?</div>
          <div class="metric-value" id="metricFooled">No</div>
        </div>
      </div>

      <div class="collapsible-section">
        <button class="collapsible-toggle" onclick="toggleDeepDive()">
          <span>üìö Technical Deep Dive: Adversarial Robustness in Medical AI</span>
          <span class="collapsible-arrow" id="deepDiveArrow">‚Ä∫</span>
        </button>
        <div class="collapsible-content deep-dive-content" id="deepDiveContent">
            <h3>What Are Adversarial Examples?</h3>
            <p>Adversarial examples are inputs to machine learning models that have been intentionally designed to cause the model to make a mistake. In medical imaging, these are perturbations to scans or diagnostic data that are imperceptible to human experts but cause AI systems to misclassify.</p>

            <h3>How Do Adversarial Attacks Work?</h3>
            <p>Adversarial attacks exploit the way neural networks process inputs:</p>
            <ul>
              <li><strong>Gradient-based attacks:</strong> Use the model's gradients to find the direction that maximally increases prediction error</li>
              <li><strong>Small perturbations:</strong> Add carefully calculated noise that humans can't detect but that pushes the input across the model's decision boundary</li>
              <li><strong>Targeted attacks:</strong> Can force the model to predict a specific incorrect class rather than just any wrong answer</li>
            </ul>

            <h3>Why This Matters for Healthcare AI</h3>
            <ul>
              <li><strong>Safety concerns:</strong> Adversarial examples could be used maliciously to manipulate diagnoses</li>
              <li><strong>Natural adversarials:</strong> Real-world data variations (scanner differences, artifacts) may inadvertently act as adversarial perturbations</li>
              <li><strong>Robustness testing:</strong> Medical AI systems must be tested against adversarial examples before deployment</li>
              <li><strong>Certification challenges:</strong> Regulatory approval requires guarantees that systems won't fail on edge cases</li>
            </ul>

            <h3>Defense Strategies</h3>
            <ul>
              <li><strong>Adversarial training:</strong> Include adversarial examples in the training data to make models more robust</li>
              <li><strong>Input preprocessing:</strong> Apply transformations that remove adversarial perturbations</li>
              <li><strong>Ensemble methods:</strong> Use multiple models to detect inconsistencies that may indicate adversarial inputs</li>
              <li><strong>Certified defenses:</strong> Mathematical guarantees that predictions won't change within a certain perturbation radius</li>
              <li><strong>Human oversight:</strong> Keep clinicians in the loop to catch suspicious AI predictions</li>
            </ul>

            <h3>Real-World Examples</h3>
            <p>Researchers have demonstrated adversarial attacks on:</p>
            <ul>
              <li>Chest X-ray classifiers (causing missed pneumonia diagnoses)</li>
              <li>Skin lesion detectors (misclassifying melanomas as benign)</li>
              <li>Retinal image analyzers (failing to detect diabetic retinopathy)</li>
              <li>CT scan interpreters (missing tumors or creating false positives)</li>
            </ul>

            <div class="key-takeaway">
              <strong>Key Takeaway:</strong> Adversarial robustness is not optional in healthcare AI. Systems must be tested against adversarial examples before deployment, and defense mechanisms (adversarial training, input validation, ensemble methods) should be incorporated. Human oversight remains essential to catch suspicious predictions that may indicate adversarial manipulation.
            </div>
        </div>
      </div>
    </div>
  </div>

  <script>
    const perturbationSlider = document.getElementById('perturbationSlider');
    const perturbationValue = document.getElementById('perturbationValue');
    const warningBox = document.getElementById('warningBox');
    const metricPerturbation = document.getElementById('metricPerturbation');
    const metricPixels = document.getElementById('metricPixels');
    const metricFooled = document.getElementById('metricFooled');
    const perturbedPrediction = document.getElementById('perturbedPrediction');

    const originalCanvas = document.getElementById('originalCanvas');
    const perturbedCanvas = document.getElementById('perturbedCanvas');
    const originalCtx = originalCanvas.getContext('2d');
    const perturbedCtx = perturbedCanvas.getContext('2d');

    // Generate base medical scan pattern
    const basePattern = [];
    for (let i = 0; i < 100; i++) {
      for (let j = 0; j < 100; j++) {
        // Create a healthy lung pattern (brighter in center, darker at edges)
        const dx = (i - 50) / 50;
        const dy = (j - 50) / 50;
        const dist = Math.sqrt(dx * dx + dy * dy);
        const base = 180 - dist * 60;
        const noise = (Math.random() - 0.5) * 20;
        basePattern.push(Math.max(80, Math.min(220, base + noise)));
      }
    }

    function drawScan(ctx, canvas, pattern, perturbation) {
      ctx.fillStyle = '#1a1a1a';
      ctx.fillRect(0, 0, canvas.width, canvas.height);

      const cellSize = canvas.width / 100;

      for (let i = 0; i < 100; i++) {
        for (let j = 0; j < 100; j++) {
          const idx = i * 100 + j;
          let value = pattern[idx];

          // Add adversarial perturbation (subtle strategic noise)
          if (perturbation > 0) {
            // Target specific pixels that affect classification
            const isTargetPixel = (i + j) % 7 === 0 || (i - j) % 5 === 0;
            if (isTargetPixel) {
              const perturbAmount = (perturbation / 100) * 40;
              value += perturbAmount * (Math.sin(i * 0.5) + Math.cos(j * 0.5));
            }
          }

          value = Math.max(0, Math.min(255, value));

          const gray = Math.floor(value);
          ctx.fillStyle = `rgb(${gray}, ${gray}, ${gray})`;
          ctx.fillRect(j * cellSize, i * cellSize, cellSize, cellSize);
        }
      }
    }

    function updateVisualization() {
      const perturbation = parseInt(perturbationSlider.value);
      perturbationValue.textContent = `${perturbation}%`;
      metricPerturbation.textContent = `${perturbation}%`;

      // Draw scans
      drawScan(originalCtx, originalCanvas, basePattern, 0);
      drawScan(perturbedCtx, perturbedCanvas, basePattern, perturbation);

      // Calculate pixels changed (approximate)
      const totalPixels = 100 * 100;
      const targetPixels = Math.floor(totalPixels * 0.3); // ~30% are target pixels
      const pixelsChanged = Math.floor((perturbation / 100) * targetPixels);
      metricPixels.textContent = pixelsChanged;

      // Update prediction based on perturbation
      let prediction = 'Healthy';
      let confidence = 92;
      let cssClass = 'healthy';
      let fooled = false;

      if (perturbation >= 45) {
        // Model starts to misclassify
        prediction = 'Disease Detected';
        confidence = 55 + (perturbation - 45) * 0.7; // Increases to ~93% at 100%
        cssClass = 'disease';
        fooled = true;
      } else if (perturbation >= 25) {
        // Uncertainty phase
        confidence = 92 - (perturbation - 25) * 2; // Drops from 92% to 52%
      }

      confidence = Math.min(95, confidence);
      metricFooled.textContent = fooled ? 'YES' : 'No';

      // Update perturbed prediction display
      perturbedPrediction.innerHTML = `
        <div class="prediction-label ${cssClass}">Prediction: ${prediction}</div>
        <div class="confidence-bar">
          <div class="confidence-fill ${cssClass}" style="width: ${confidence}%;">${Math.round(confidence)}% Confidence</div>
        </div>
      `;

      // Update warning box
      if (perturbation === 0) {
        warningBox.innerHTML = '';
      } else if (perturbation < 25) {
        warningBox.innerHTML = `
          <div class="warning-box">
            <div class="warning-title">‚ö†Ô∏è Perturbation Applied</div>
            <div class="warning-text">Small changes added, but model remains confident in correct prediction.</div>
          </div>
        `;
      } else if (perturbation < 45) {
        warningBox.innerHTML = `
          <div class="warning-box">
            <div class="warning-title">‚ö†Ô∏è Model Uncertainty Increasing</div>
            <div class="warning-text">Confidence is dropping as adversarial noise approaches decision boundary.</div>
          </div>
        `;
      } else {
        warningBox.innerHTML = `
          <div class="warning-box danger">
            <div class="warning-title">üö® Model Fooled!</div>
            <div class="warning-text">Adversarial perturbation has successfully caused misclassification. The model now predicts disease in a healthy scan.</div>
          </div>
        `;
      }
    }

    function toggleDeepDive() {
      const content = document.getElementById('deepDiveContent');
      const arrow = document.getElementById('deepDiveArrow');
      content.classList.toggle('visible');
      arrow.classList.toggle('open');
    }

    perturbationSlider.addEventListener('input', updateVisualization);

    // Initial render
    updateVisualization();
  </script>
</body>
</html>
